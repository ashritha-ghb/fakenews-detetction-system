{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nlp\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b016d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "df_true = pd.read_csv(\"True.csv\")\n",
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39760e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true[\"text\"] = df_true[\"text\"].replace(\"(Reuters)\",\"\",regex=True)\n",
    "df_true.head()\n",
    "## The String \"()\" still remains in the text column to be removed which i take care of during the NLP part done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"target\"] = 0\n",
    "df_true[\"target\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = df_fake.drop([\"title\",\"subject\",\"date\"],axis = 1)\n",
    "df_true = df_true.drop([\"title\",\"subject\",\"date\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f88fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_fake,df_true],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop([\"index\"], axis = 1, inplace = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    # This is where i remove the \"()\" from the text column. You can do in whatever way you want\n",
    "    # The key is to remove the \"(Reuters)\" string as it is present in all text of True.csv.\n",
    "    # The Model during the training part can memorize it and perfrom great in training and badly when other testing input is given.\n",
    "    text = re.sub('[()]','',text)\n",
    "    text = re.sub('\\\\W',' ',text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a07ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(wordopt)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef07dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "Y = df[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3582f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,x_test,Y_train,y_test = train_test_split(X,Y,test_size=0.25)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417890f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#print(X_train)\n",
    "vectorization = TfidfVectorizer()\n",
    "analyze = vectorization.build_analyzer()\n",
    "#print(analyze(X_train[0]))\n",
    "xv_train = vectorization.fit_transform(X_train)\n",
    "xv_test = vectorization.transform(x_test)\n",
    "print(xv_train.shape)\n",
    "print(xv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(xv_train,Y_train)\n",
    "print(\"The Accuracy of the Logistic Regression Model is {}\".format(lr.score(xv_test,y_test)))\n",
    "print(classification_report(y_test,lr.predict(xv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(xv_train,Y_train)\n",
    "print(\"The Accuracy of the Decision Tree Classifier Model is {}\".format(dtc.score(xv_test,y_test)))\n",
    "print(classification_report(y_test,dtc.predict(xv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df049491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gclf = GradientBoostingClassifier()\n",
    "gclf.fit(xv_train,Y_train)\n",
    "print(\"The Accuracy of the Gradient Boosting Classifier Model is {}\".format(gclf.score(xv_test,y_test)))\n",
    "print(classification_report(y_test,gclf.predict(xv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204202f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rclf = RandomForestClassifier()\n",
    "rclf.fit(xv_train,Y_train)\n",
    "print(\"The Accuracy of the Random Forest Classifier Model is {}\".format(rclf.score(xv_test,y_test)))\n",
    "print(classification_report(y_test,rclf.predict(xv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff420fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lable(n):\n",
    "    if n == 0:\n",
    "        return \"Fake News\"\n",
    "    elif n == 1:\n",
    "        return \"Not A Fake News\"\n",
    "\n",
    "def manual_testing(news):\n",
    "    testing_news = {\"text\":[news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wordopt)\n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    new_xv_test = vectorization.transform(new_x_test)\n",
    "    pred_LR = lr.predict(new_xv_test)\n",
    "    pred_DT = dtc.predict(new_xv_test)\n",
    "    pred_GBC = gclf.predict(new_xv_test)\n",
    "    pred_RFC = rclf.predict(new_xv_test)\n",
    "\n",
    "    return print(\"\\n\\nLR Prediction: {} \\nDT Prediction: {} \\nGBC Prediction: {} \\nRFC Prediction: {}\".format(output_lable(pred_LR[0]),                                                                                                       output_lable(pred_DT[0]),\n",
    "                                                                                                              output_lable(pred_GBC[0]),\n",
    "                                                                                                              output_lable(pred_RFC[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53df30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save all models\n",
    "joblib.dump(lr, 'lr_model.jb')       # Logistic Regression\n",
    "joblib.dump(dtc, 'dtc_model.jb')     # Decision Tree Classifier\n",
    "joblib.dump(gclf, 'gclf_model.jb')   # Gradient Boosting Classifier\n",
    "joblib.dump(rclf, 'rclf_model.jb')   # Random Forest Classifier\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorization, 'vectorizer.jb')\n",
    "\n",
    "print(\"All models and the vectorizer have been saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "\n",
    "# Set style for all plots\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Visualization 1: Model Performance Comparison\n",
    "models = ['Logistic Regression', 'Decision Tree', 'Gradient Boosting', 'Random Forest']\n",
    "accuracy = [lr.score(xv_test,y_test), dtc.score(xv_test,y_test), \n",
    "            gclf.score(xv_test,y_test), rclf.score(xv_test,y_test)]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "bars = plt.bar(models, accuracy, color=['#4e79a7', '#f28e2b', '#e15759', '#76b7b2'])\n",
    "plt.title('Model Accuracy Comparison', fontsize=16)\n",
    "plt.xlabel('Models', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.ylim(0.8, 1.0)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}',\n",
    "             ha='center', va='bottom')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Confusion Matrix (Matplotlib version if seaborn not available)\n",
    "lr_pred = lr.predict(xv_test)\n",
    "cm = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=16)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Fake', 'Real'], fontsize=12)\n",
    "plt.yticks(tick_marks, ['Fake', 'Real'], fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('Actual', fontsize=14)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 3: Feature Importance (Top 20 words)\n",
    "feature_names = vectorization.get_feature_names_out()\n",
    "coef = lr.coef_[0]\n",
    "top_20 = np.argsort(coef)[-20:]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh([feature_names[i] for i in top_20], [coef[i] for i in top_20], color='#59a14f')\n",
    "plt.title('Top 20 Words Indicating Real News', fontsize=16)\n",
    "plt.xlabel('Coefficient Value', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 4: Class Distribution\n",
    "plt.figure(figsize=(8,6))\n",
    "class_counts = df['target'].value_counts()\n",
    "plt.pie(class_counts, autopct='%1.1f%%', \n",
    "       colors=['#e15759', '#4e79a7'],\n",
    "       labels=['Fake', 'Real'],\n",
    "       startangle=90)\n",
    "plt.title('Distribution of Fake vs Real News', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 5: Precision-Recall Curve\n",
    "probs = lr.predict_proba(xv_test)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(recall, precision, marker='.', color='#b07aa1', linewidth=2)\n",
    "plt.title('Precision-Recall Curve', fontsize=16)\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 6: Text Length Analysis (Matplotlib version)\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "fake_lengths = df[df['target'] == 0]['text_length']\n",
    "real_lengths = df[df['target'] == 1]['text_length']\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot([fake_lengths, real_lengths], \n",
    "            labels=['Fake', 'Real'],\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor='#4e79a7', color='k'),\n",
    "            medianprops=dict(color='k'))\n",
    "plt.title('Text Length Distribution by News Type', fontsize=16)\n",
    "plt.xlabel('News Type', fontsize=14)\n",
    "plt.ylabel('Text Length (log scale)', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "venv",
=======
   "display_name": "Python 3",
>>>>>>> 321b0f20b67eed51dabdfc2e2b9e80fd2b37e95b
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
